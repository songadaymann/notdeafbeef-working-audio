	.text
	.align 2
	.globl _generator_mix_buffers_asm

/*
 * generator_mix_buffers_asm - NEON vectorized buffer mixing
 * -------------------------------------------------------
 * void generator_mix_buffers_asm(float *L, float *R, 
 *                                 const float *Ld, const float *Rd,
 *                                 const float *Ls, const float *Rs,
 *                                 uint32_t num_frames);
 *
 * Performs: L[i] = Ld[i] + Ls[i]  (drums + synths)
 *          R[i] = Rd[i] + Rs[i]  (drums + synths)
 *
 * Uses NEON to process 4 samples per iteration for maximum throughput.
 * This is the hot path that runs every audio frame in real-time.
 */

_generator_mix_buffers_asm:
	// Arguments: x0=L, x1=R, x2=Ld, x3=Rd, x4=Ls, x5=Rs, w6=num_frames
	
	// Early exit if no frames to process
	cbz w6, .Lmix_done
	
	// Calculate how many complete NEON vectors (4 samples) we can process
	lsr w7, w6, #2          // w7 = num_frames / 4 (complete vectors)
	and w8, w6, #3          // w8 = num_frames % 4 (remainder samples)
	
	// Process complete 4-sample vectors with NEON
	cbz w7, .Lmix_scalar    // Skip if no complete vectors
	
.Lmix_vector_loop:
	// Load 4 samples from each source buffer
	ld1 {v0.4s}, [x2], #16  // v0 = Ld[i..i+3], advance pointer
	ld1 {v1.4s}, [x3], #16  // v1 = Rd[i..i+3], advance pointer  
	ld1 {v2.4s}, [x4], #16  // v2 = Ls[i..i+3], advance pointer
	ld1 {v3.4s}, [x5], #16  // v3 = Rs[i..i+3], advance pointer
	
	// Vector addition: drums + synths
	fadd v4.4s, v0.4s, v2.4s  // v4 = Ld + Ls
	fadd v5.4s, v1.4s, v3.4s  // v5 = Rd + Rs
	
	// Store results to output buffers
	st1 {v4.4s}, [x0], #16   // L[i..i+3] = v4, advance pointer
	st1 {v5.4s}, [x1], #16   // R[i..i+3] = v5, advance pointer
	
	// Loop control
	subs w7, w7, #1
	b.ne .Lmix_vector_loop
	
.Lmix_scalar:
	// Handle remaining samples (0-3) with scalar operations
	cbz w8, .Lmix_done
	
.Lmix_scalar_loop:
	// Load single samples
	ldr s0, [x2], #4        // s0 = Ld[i]
	ldr s1, [x3], #4        // s1 = Rd[i]
	ldr s2, [x4], #4        // s2 = Ls[i]
	ldr s3, [x5], #4        // s3 = Rs[i]
	
	// Scalar addition
	fadd s4, s0, s2         // s4 = Ld[i] + Ls[i]
	fadd s5, s1, s3         // s5 = Rd[i] + Rs[i]
	
	// Store results
	str s4, [x0], #4        // L[i] = s4
	str s5, [x1], #4        // R[i] = s5
	
	// Loop control
	subs w8, w8, #1
	b.ne .Lmix_scalar_loop
	
.Lmix_done:
	ret

/*
 * generator_compute_rms_asm - NEON vectorized RMS calculation
 * ---------------------------------------------------------
 * float generator_compute_rms_asm(const float *L, const float *R, uint32_t num_frames);
 *
 * Computes RMS = sqrt(sum(L[i]² + R[i]²) / (num_frames * 2))
 * 
 * Uses NEON to process 4 samples per iteration:
 * - Load L[i..i+3] and R[i..i+3] 
 * - Square each (fmul)
 * - Add L² + R² (fadd)
 * - Accumulate in vector sum
 * - Final horizontal sum + sqrt in scalar
 */

	.globl _generator_compute_rms_asm

_generator_compute_rms_asm:
	// Arguments: x0=L, x1=R, w2=num_frames
	// Returns: s0 = RMS value
	
	// Early exit if no frames
	cbz w2, .Lrms_zero
	
	// Initialize accumulator vector to zero
	movi v16.4s, #0              // v16 = accumulator for vector sum
	fmov s17, wzr                // s17 = accumulator for scalar sum
	
	// Calculate how many complete NEON vectors (4 samples) we can process
	lsr w3, w2, #2               // w3 = num_frames / 4 (complete vectors)
	and w4, w2, #3               // w4 = num_frames % 4 (remainder samples)
	
	// Process complete 4-sample vectors with NEON
	cbz w3, .Lrms_scalar         // Skip if no complete vectors
	
.Lrms_vector_loop:
	// Load 4 samples from each buffer
	ld1 {v0.4s}, [x0], #16       // v0 = L[i..i+3], advance pointer
	ld1 {v1.4s}, [x1], #16       // v1 = R[i..i+3], advance pointer
	
	// Square the samples: L² and R²
	fmul v2.4s, v0.4s, v0.4s     // v2 = L[i]² for 4 samples
	fmul v3.4s, v1.4s, v1.4s     // v3 = R[i]² for 4 samples
	
	// Add L² + R² 
	fadd v4.4s, v2.4s, v3.4s     // v4 = L[i]² + R[i]² for 4 samples
	
	// Accumulate in sum vector
	fadd v16.4s, v16.4s, v4.4s   // accumulate
	
	// Loop control
	subs w3, w3, #1
	b.ne .Lrms_vector_loop
	
.Lrms_scalar:
	// Handle remaining samples (0-3) with scalar operations
	cbz w4, .Lrms_finalize
	
.Lrms_scalar_loop:
	// Load single samples
	ldr s0, [x0], #4             // s0 = L[i]
	ldr s1, [x1], #4             // s1 = R[i]
	
	// Square and add: L² + R²
	fmul s2, s0, s0              // s2 = L[i]²
	fmul s3, s1, s1              // s3 = R[i]²
	fadd s4, s2, s3              // s4 = L[i]² + R[i]²
	
	// Add to scalar accumulator  
	fadd s17, s17, s4            // accumulate scalar remainder
	
	// Loop control
	subs w4, w4, #1
	b.ne .Lrms_scalar_loop
	
.Lrms_finalize:
	// Horizontal sum of accumulator vector v16 → s0
	faddp v18.4s, v16.4s, v16.4s // pairwise add: [a+b, c+d, a+b, c+d]
	faddp s0, v18.2s             // final vector sum: (a+b) + (c+d)
	
	// Add scalar accumulator to vector sum
	fadd s0, s0, s17             // total_sum = vector_sum + scalar_sum
	
	// Convert num_frames to float and multiply by 2
	ucvtf s1, w2                 // s1 = (float)num_frames  
	fmov s2, #2.0                // s2 = 2.0
	fmul s1, s1, s2              // s1 = num_frames * 2
	
	// Divide sum by (num_frames * 2) to get mean
	fdiv s0, s0, s1              // s0 = mean = sum / (num_frames * 2)
	
	// Take square root to get RMS
	fsqrt s0, s0                 // s0 = sqrt(mean) = RMS
	
	ret
	
.Lrms_zero:
	// Return 0.0 if no frames
	fmov s0, wzr
	ret 

/*
 * generator_clear_buffers_asm - NEON vectorized buffer clearing
 * -----------------------------------------------------------
 * void generator_clear_buffers_asm(float *Ld, float *Rd, float *Ls, float *Rs, uint32_t num_frames);
 *
 * Clears (zeros) all 4 float buffers using NEON vector stores.
 * Replaces 4 memset() calls with optimized NEON operations.
 * 
 * Uses NEON to process 4 samples per iteration for maximum throughput.
 */

	.globl _generator_clear_buffers_asm

_generator_clear_buffers_asm:
	// Arguments: x0=Ld, x1=Rd, x2=Ls, x3=Rs, w4=num_frames
	
	// Early exit if no frames to process
	cbz w4, .Lclear_done
	
	// Initialize zero vector for NEON stores
	movi v0.4s, #0               // v0 = [0.0, 0.0, 0.0, 0.0]
	
	// Calculate how many complete NEON vectors (4 samples) we can process
	lsr w5, w4, #2               // w5 = num_frames / 4 (complete vectors)
	and w6, w4, #3               // w6 = num_frames % 4 (remainder samples)
	
	// Process complete 4-sample vectors with NEON
	cbz w5, .Lclear_scalar       // Skip if no complete vectors
	
.Lclear_vector_loop:
	// Store 4 zero samples to each buffer
	st1 {v0.4s}, [x0], #16       // Ld[i..i+3] = 0.0, advance pointer
	st1 {v0.4s}, [x1], #16       // Rd[i..i+3] = 0.0, advance pointer  
	st1 {v0.4s}, [x2], #16       // Ls[i..i+3] = 0.0, advance pointer
	st1 {v0.4s}, [x3], #16       // Rs[i..i+3] = 0.0, advance pointer
	
	// Loop control
	subs w5, w5, #1
	b.ne .Lclear_vector_loop
	
.Lclear_scalar:
	// Handle remaining samples (0-3) with scalar operations
	cbz w6, .Lclear_done
	
	// Zero value for scalar stores
	fmov s1, wzr                 // s1 = 0.0
	
.Lclear_scalar_loop:
	// Store single zero sample to each buffer
	str s1, [x0], #4             // Ld[i] = 0.0
	str s1, [x1], #4             // Rd[i] = 0.0
	str s1, [x2], #4             // Ls[i] = 0.0  
	str s1, [x3], #4             // Rs[i] = 0.0
	
	// Loop control
	subs w6, w6, #1
	b.ne .Lclear_scalar_loop
	
.Lclear_done:
	ret

/*
 * generator_rotate_pattern_asm - NEON vectorized pattern rotation
 * -------------------------------------------------------------
 * void generator_rotate_pattern_asm(uint8_t *pattern, uint8_t *tmp, uint32_t size, uint32_t rot);
 *
 * Rotates a uint8_t array by 'rot' positions: pattern[i] = old_pattern[(i+rot) % size]
 * Optimized for size=16 (STEPS_PER_BAR) using NEON EXT instruction.
 * 
 * For size=16: Single NEON register holds entire pattern, EXT performs rotation in one operation.
 */

	.globl _generator_rotate_pattern_asm

_generator_rotate_pattern_asm:
	// Arguments: x0=pattern, x1=tmp, w2=size, w3=rot
	
	// Early exit if no rotation needed
	cbz w3, .Lrot_done
	
	// Optimize for STEPS_PER_BAR = 16 case
	cmp w2, #16
	b.eq .Lrot_neon16
	
	// Fallback for other sizes: scalar implementation
	b .Lrot_scalar
	
.Lrot_neon16:
	// NEON optimization for 16-byte patterns (STEPS_PER_BAR)
	// Load entire 16-byte pattern into single NEON register
	ld1 {v0.16b}, [x0]
	
	// Build rotation index table: [rot, rot+1, rot+2, ..., rot+15] % 16
	and w3, w3, #15          // Ensure rot is 0-15 (rot % 16)
	
	// Build index table on stack
	sub sp, sp, #16          // Allocate 16 bytes on stack
	mov w4, wzr              // w4 = loop counter
	
.Lrot_build_indices:
	add w5, w4, w3           // w5 = i + rot
	and w5, w5, #15          // w5 = (i + rot) % 16
	strb w5, [sp, w4, uxtw]  // Store index to stack
	add w4, w4, #1           // i++
	cmp w4, #16
	b.lt .Lrot_build_indices
	
	// Load index table and use TBL for rotation
	ld1 {v2.16b}, [sp]       // Load index table
	tbl v1.16b, {v0.16b}, v2.16b  // Perform table lookup rotation
	
	// Store rotated pattern back and clean up stack
	st1 {v1.16b}, [x0]
	add sp, sp, #16          // Restore stack
	
	b .Lrot_done
	
.Lrot_scalar:
	// Scalar fallback for arbitrary sizes
	// Copy pattern to tmp buffer
	mov w4, wzr              // w4 = loop counter for memcpy
	
.Lrot_copy_loop:
	cmp w4, w2
	b.ge .Lrot_rotate_start
	ldrb w5, [x0, w4, uxtw]  // Load pattern[i]
	strb w5, [x1, w4, uxtw]  // Store to tmp[i]
	add w4, w4, #1
	b .Lrot_copy_loop
	
.Lrot_rotate_start:
	// Rotate: pattern[i] = tmp[(i+rot) % size]
	mov w4, wzr              // w4 = i (loop counter)
	
.Lrot_rotate_loop:
	cmp w4, w2
	b.ge .Lrot_done
	
	// Calculate src_index = (i + rot) % size
	add w5, w4, w3           // w5 = i + rot
	udiv w6, w5, w2          // w6 = (i + rot) / size
	msub w5, w6, w2, w5      // w5 = (i + rot) - (w6 * size) = (i + rot) % size
	
	// pattern[i] = tmp[src_index]
	ldrb w6, [x1, w5, uxtw]  // Load tmp[src_index]
	strb w6, [x0, w4, uxtw]  // Store to pattern[i]
	
	add w4, w4, #1           // i++
	b .Lrot_rotate_loop
	
.Lrot_done:
	ret 

/*
 * generator_build_events_asm - Pre-compute entire event queue in assembly
 * ----------------------------------------------------------------------
 * void generator_build_events_asm(event_queue_t *q, rng_t *rng, 
 *                                  const uint8_t *kick_pat, const uint8_t *snare_pat, const uint8_t *hat_pat,
 *                                  uint32_t step_samples);
 *
 * Converts the event queue building loop from C to assembly for ultimate performance.
 * This is the final orchestration step - building the complete musical timeline.
 *
 * Event generation rules:
 * - Drums: kick/snare/hat based on euclidean patterns
 * - Melody: triggers at specific bar positions (0, 8, 16, 24) 
 * - Mid: stochastic triggers with 10% probability on certain beats
 * - Bass: triggers at beginning of each bar (step 0)
 *
 * Constants:
 * - TOTAL_STEPS = 32, STEPS_PER_BAR = 16
 * - Event types: KICK=0, SNARE=1, HAT=2, MELODY=3, MID=4, FM_BASS=5
 */

	.globl _generator_build_events_asm

_generator_build_events_asm:
	// Arguments: x0=q, x1=rng, x2=kick_pat, x3=snare_pat, x4=hat_pat, w5=step_samples
	
	// Save callee-saved registers
	stp x19, x20, [sp, #-80]!
	stp x21, x22, [sp, #16]
	stp x23, x24, [sp, #32]
	stp x25, x26, [sp, #48]
	stp x27, x28, [sp, #64]
	
	// Initialize event queue: q->count = 0
	str wzr, [x0, #4096]        // q->count = 0 (events array is 4096 bytes)
	
	// Register assignments for loop
	mov x19, x0                 // x19 = q (event queue)
	mov x20, x1                 // x20 = rng
	mov x21, x2                 // x21 = kick_pat
	mov x22, x3                 // x22 = snare_pat
	mov x23, x4                 // x23 = hat_pat
	mov w24, w5                 // w24 = step_samples
	mov w25, wzr                // w25 = step (loop counter)
	
	// Constants
	mov w26, #32                // w26 = TOTAL_STEPS
	mov w27, #16                // w27 = STEPS_PER_BAR
	
	// RNG_FLOAT constants - removed unused constant
	
.Lbuild_loop:
	// Check loop condition: step < TOTAL_STEPS
	cmp w25, w26
	b.ge .Lbuild_done
	
	// Calculate t = step * step_samples
	mul w6, w25, w24            // w6 = t = step * step_samples
	
	// Calculate bar_step = step % STEPS_PER_BAR
	udiv w7, w25, w27           // w7 = step / STEPS_PER_BAR
	msub w8, w7, w27, w25       // w8 = bar_step = step - (w7 * STEPS_PER_BAR)
	
	// Check kick pattern: if(kick_pat[step % STEPS_PER_BAR])
	ldrb w9, [x21, w8, uxtw]    // w9 = kick_pat[bar_step]
	cbz w9, .Lcheck_snare
	
	// Push kick event: eq_push(q, t, EVT_KICK, 0)
	mov w10, #0                 // EVT_KICK = 0
	mov w11, #0                 // aux = 0
	bl _generator_eq_push_helper_asm
	
.Lcheck_snare:
	// Check snare pattern: if(snare_pat[step % STEPS_PER_BAR])
	ldrb w9, [x22, w8, uxtw]    // w9 = snare_pat[bar_step]
	cbz w9, .Lcheck_hat
	
	// Push snare event: eq_push(q, t, EVT_SNARE, 0)
	mov w10, #1                 // EVT_SNARE = 1
	mov w11, #0                 // aux = 0
	bl _generator_eq_push_helper_asm
	
.Lcheck_hat:
	// Check hat pattern: if(hat_pat[step % STEPS_PER_BAR])
	ldrb w9, [x23, w8, uxtw]    // w9 = hat_pat[bar_step]
	cbz w9, .Lcheck_melody
	
	// Push hat event: eq_push(q, t, EVT_HAT, 0)
	mov w10, #2                 // EVT_HAT = 2
	mov w11, #0                 // aux = 0
	bl _generator_eq_push_helper_asm
	
.Lcheck_melody:
	// Check melody triggers: if(bar_step==0 || bar_step==8 || bar_step==16 || bar_step==24)
	cbz w8, .Lmelody_trigger    // bar_step == 0
	cmp w8, #8
	b.eq .Lmelody_trigger
	cmp w8, #16
	b.eq .Lmelody_trigger
	cmp w8, #24
	b.eq .Lmelody_trigger
	b .Lcheck_mid
	
.Lmelody_trigger:
	// Push melody event: eq_push(q, t, EVT_MELODY, bar_step/8)
	lsr w11, w8, #3             // w11 = aux = bar_step / 8
	mov w10, #3                 // EVT_MELODY = 3
	bl _generator_eq_push_helper_asm
	
.Lcheck_mid:
	// Check mid triggers: if(bar_step % 4 == 2 || ((bar_step%4==1 || bar_step%4==3) && RNG_FLOAT < 0.1))
	and w9, w8, #3              // w9 = bar_step % 4
	cmp w9, #2
	b.eq .Lmid_trigger          // bar_step % 4 == 2
	
	// Check if bar_step % 4 == 1 or 3
	cmp w9, #1
	b.eq .Lmid_rng_check
	cmp w9, #3
	b.ne .Lcheck_bass
	
.Lmid_rng_check:
	// Generate RNG_FLOAT and compare with 0.1
	bl _generator_rng_next_float_asm   // Returns float in s0
	
	// Compare with 0.1f
	mov w12, #0x3dcc
	movk w12, #0xcccd, lsl #16  // 0.1f in IEEE 754
	fmov s1, w12
	fcmp s0, s1
	b.ge .Lcheck_bass           // if RNG_FLOAT >= 0.1, skip
	
.Lmid_trigger:
	// Generate random aux value: rng_next_u32() % 7
	bl _generator_rng_next_u32_asm   // Returns uint32_t in w0
	mov w12, #7
	udiv w13, w0, w12
	msub w11, w13, w12, w0      // w11 = aux = w0 % 7
	
	// Push mid event: eq_push(q, t, EVT_MID, aux)
	mov w10, #4                 // EVT_MID = 4
	bl _generator_eq_push_helper_asm
	
.Lcheck_bass:
	// Check bass trigger: if(bar_step == 0)
	cbnz w8, .Lloop_next
	
	// Push bass event: eq_push(q, t, EVT_FM_BASS, 0)
	mov w10, #5                 // EVT_FM_BASS = 5
	mov w11, #0                 // aux = 0
	bl _generator_eq_push_helper_asm
	
.Lloop_next:
	// Increment step and continue loop
	add w25, w25, #1
	b .Lbuild_loop
	
.Lbuild_done:
	// Restore callee-saved registers
	ldp x27, x28, [sp, #64]
	ldp x25, x26, [sp, #48]
	ldp x23, x24, [sp, #32]
	ldp x21, x22, [sp, #16]
	ldp x19, x20, [sp], #80
	ret

/*
 * Helper function: eq_push equivalent
 * Inputs: w6=time, w10=type, w11=aux
 * Uses: x19=q
 */
	.globl _generator_eq_push_helper_asm
_generator_eq_push_helper_asm:
	// Load current count
	ldr w12, [x19, #4096]       // w12 = q->count
	
	// Check if count < MAX_EVENTS (512)
	cmp w12, #512
	b.ge .Leq_push_ret          // Skip if queue full
	
	// Calculate event address: &q->events[count]
	mov w13, #8                 // sizeof(event_t) = 8 bytes
	mul w14, w12, w13           // w14 = count * sizeof(event_t)
	add x15, x19, w14, uxtw     // x15 = &q->events[count]
	
	// Store event: {time, type, aux, padding}
	str w6, [x15]               // event.time = time
	strb w10, [x15, #4]         // event.type = type
	strb w11, [x15, #5]         // event.aux = aux
	
	// Increment count
	add w12, w12, #1
	str w12, [x19, #4096]       // q->count++
	
.Leq_push_ret:
	ret

/*
 * Helper function: rng_next_u32 equivalent
 * Inputs: x20=rng
 * Returns: w0=random uint32_t
 * Preserves: x20 (rng pointer)
 */
	.globl _generator_rng_next_u32_asm  
_generator_rng_next_u32_asm:
	// Save link register and preserve registers
	stp x29, x30, [sp, #-16]!
	
	// Implementation of SplitMix64 algorithm
	// uint64_t z = (r->state += 0x9E3779B97F4A7C15ULL);
	ldr x0, [x20]               // x0 = rng->state
	movz x1, #0x7C15, lsl #0
	movk x1, #0x7F4A, lsl #16
	movk x1, #0xB979, lsl #32
	movk x1, #0x9E37, lsl #48   // x1 = 0x9E3779B97F4A7C15
	add x0, x0, x1              // x0 = state + increment
	str x0, [x20]               // rng->state = new state
	
	// z = (z ^ (z >> 30)) * 0xBF58476D1CE4E5B9ULL;
	lsr x1, x0, #30
	eor x0, x0, x1
	movz x1, #0xE5B9, lsl #0
	movk x1, #0x1CE4, lsl #16
	movk x1, #0x476D, lsl #32
	movk x1, #0xBF58, lsl #48   // x1 = 0xBF58476D1CE4E5B9
	mul x0, x0, x1
	
	// z = (z ^ (z >> 27)) * 0x94D049BB133111EBULL;
	lsr x1, x0, #27
	eor x0, x0, x1
	movz x1, #0x11EB, lsl #0
	movk x1, #0x3311, lsl #16
	movk x1, #0x49BB, lsl #32
	movk x1, #0x94D0, lsl #48   // x1 = 0x94D049BB133111EB
	mul x0, x0, x1
	
	// return z ^ (z >> 31);
	lsr x1, x0, #31
	eor x0, x0, x1
	
	// Return lower 32 bits
	mov w0, w0
	
	// Restore and return
	ldp x29, x30, [sp], #16
	ret

/*
 * Helper function: rng_next_float equivalent  
 * Inputs: x20=rng
 * Returns: s0=random float [0,1)
 */
	.globl _generator_rng_next_float_asm
_generator_rng_next_float_asm:
	// Save link register and floating-point context
	stp x29, x30, [sp, #-16]!
	
	// Call rng_next_u32
	bl _generator_rng_next_u32_asm   // w0 = random uint32_t
	
	// Implement: (rng_next_u32(r) >> 8) * (1.0f / 16777216.0f)
	lsr w0, w0, #8              // w0 = w0 >> 8
	ucvtf s0, w0                // s0 = (float)w0
	
	// Multiply by 1.0f / 16777216.0f = 5.960464477539063e-08
	movz w1, #0x0000, lsl #0
	movk w1, #0x3380, lsl #16   // IEEE 754 representation of 1.0f/16777216.0f
	fmov s1, w1
	fmul s0, s0, s1             // s0 = s0 * (1.0f/16777216.0f)
	
	// Restore and return
	ldp x29, x30, [sp], #16
	ret 